dataset:
    name: 'sbm'
    remove_h: null
    datadir: 'data/sbm/'
    rand_perm: False
    feat:
        type:
            - deg
            - eig1
            - eig2
        scale: 10
        norm: True

general:
    # General settings
    name: 'graph-tf-model'      # Warning: 'debug' and 'test' are reserved name that have a special behavior

    process_visualization: False
    sample_visualization: False
    vis_number: 30

    wandb: 'disabled'             # online | offline | disabled
    gpus: [3,]                     # Multi-gpu is not implemented on this branch

    resume: # '/mnt/d/lxy/graphgen/GBD/outputs/sbm/2024-08-01/11-52-17-graph-tf-model/checkpoints/graph-tf-model/epoch=15999-v1.ckpt'  # If resume, path to ckpt file from outputs directory in main directory
    ema_resume: # '/mnt/d/lxy/graphgen/GBD/outputs/sbm/2024-08-01/11-52-17-graph-tf-model/checkpoints/graph-tf-model/15999_ema.pth'       
    test_only: '/mnt/d/lxy/graphgen/GBD/outputs/sbm/2024-08-07/16-04-55-graph-tf-model/checkpoints/graph-tf-model/epoch=27999-v1.ckpt'

    check_val_every_n_epochs: 2000
    sample_every_val: 1
    val_check_interval: null
    samples_to_generate: 40       # We advise to set it to 2 x batch_size maximum
    samples_to_save: 40
    chains_to_save: 1
    log_every_steps: 50
    number_chain_steps: 50        # Number of frames in each gif

    final_model_samples_to_generate: 120
    final_model_samples_to_save: 20
    final_model_chains_to_save: 20
    evaluate_all_checkpoints: False

train:
    # Training settings
    n_epochs: 100000
    batch_size: 8
    lr: 0.0002
    clip_grad: 1.0          # float, null to disable
    save_model: True
    num_workers: 0
    ema_decay: 0.999           # 'Amount of EMA decay, 0 means off. A reasonable value  is 0.999.'
    progress_bar: false
    weight_decay: 1e-12
    optimizer: adamw # adamw,nadamw,nadam => nadamw for large batches, see http://arxiv.org/abs/2102.06356 for the use of nesterov momentum with large batches
    seed: 0

model:
    # Model settings
    type: 'discrete'
    transition: 'marginal'                          # uniform or marginal
    model: 'graph_tf'
    diffusion_steps: 1000
    diffusion_noise_schedule: 'cosine'              # 'cosine', 'polynomial_2'
    n_layers: 8

    extra_features:          # 'all', 'cycles', 'eigenvalues' or null

    # Do not set hidden_mlp_E, dim_ffE too high, computing large tensors on the edges is costly
    # At the moment (03/08), y contains quite little information
    hidden_mlp_dims: {'X': 128, 'E': 64, 'y': 128}

    # The dimensions should satisfy dx % n_head == 0
    hidden_dims : {'dx': 256, 'de': 64, 'dy': 64, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 64, 'dim_ffy': 256}

    high_order: False
    # DruM Setting
    # hidden_mlp_dims: {'X': 128, 'E': 64, 'y': 128}
    # hidden_dims : {'dx': 256, 'de': 64, 'dy': 64, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 64, 'dim_ffy': 256}

    lambda_train: [5, 0]

    sigmoid_start: 10    # 9
    sigmoid_end: -13     # -9
    sigmoid_power: 1.  # 0.5
    scale_shift:
        node: [0.39, 0.6]    # originally 0.39
        edge: [0.9, 0.09]    # originally 0.6

    eta:
        node: [10000, 10000, 10000, 10000]
        edge: [1000, 1000, 1000, 1000]
    input_space: 'logit' # 'logit' or 'original'
    pre_condition: True

    noise_feat_type: 'eig' # 'deg', 'eig', 'all', None
    default_min_max: #[-1., 1.]  # None
    max_feat_num: [13, 1, 1]

    cibcentration_m: False
    concentration_strategy: 'deg' # 'deg', 'central', 'betweenness'
    
hydra:
  job:
    chdir: True
  run:
    dir: ./outputs/${dataset.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}-${general.name}
